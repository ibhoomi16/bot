# bot-detector/client.py

import os
import pandas as pd
import json
import re
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from tqdm import tqdm
import pickle
from cryptography.fernet import Fernet # For symmetric encryption

# --- Configuration ---
BASE_PARTITION_DIR = 'dataset/partition' # Relative path to the partition folder
PHASE = 'phase1' # Using phase1 as per our detailed discussions

# NEW: Directory for client updates
CLIENT_UPDATES_DIR = 'client_updates'

# --- Encryption Key (IMPORTANT: In a real system, this key should be securely shared/managed) ---
# You MUST replace this with the key generated by running scripts/key.py
# Example: b'jR8oXfDmf9JOV2PzXzCL_0tPr8YzUwg-9Akem3CpuAc='
ENCRYPTION_KEY = b'BxvBWlI4M2KYqy_q0ituuVCxq-sibLYhCyYFJlxYuRc=' # Placeholder, replace with your actual key

# --- Differential Privacy Configuration ---
DP_NOISE_SCALE = 0.1 # Adjust this value: higher means more privacy, but less utility

# --- Helper Functions (No changes to these) ---
def parse_web_log_entry(log_entry):
    """Parses a single Apache-like web log entry to extract relevant features."""
    match = re.match(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) - - \[(.*?)\] "(.*?)" (\d+) (\d+) "(.*?)" "(.*?)" "PHPSESSID=(.*?)"', log_entry)
    if match:
        ip_address, timestamp_str, request_line, status, bytes_sent, referer, user_agent, session_id = match.groups()
        method, path, http_version = request_line.split(' ', 2)
        return {
            'session_id': session_id,
            'ip_address': ip_address,
            'timestamp_str': timestamp_str,
            'method': method,
            'path': path,
            'status_code': int(status),
            'bytes_sent': int(bytes_sent) if bytes_sent.isdigit() else 0,
            'referer': referer,
            'user_agent': user_agent
        }
    return None

def extract_mouse_movement_features(mouse_data):
    """Extracts features from the mouse movement JSON data (phase1 structure)."""
    features = {}
    total_behaviour = mouse_data.get('total_behaviour', [])
    mousemove_times = mouse_data.get('mousemove_times', [])
    mousemove_total_behaviour = mouse_data.get('mousemove_total_behaviour', [])

    features['num_moves'] = total_behaviour.count('m')
    features['num_left_clicks'] = total_behaviour.count('c(l)')
    features['num_right_clicks'] = total_behaviour.count('c(r)')
    features['num_middle_clicks'] = total_behaviour.count('c(m)')
    features['total_actions'] = len(total_behaviour)

    times_numeric = []
    if mousemove_times:
        times_numeric = [float(t.strip('()')) for t in mousemove_times if t.strip('()').replace('.', '', 1).isdigit()]

    if len(times_numeric) > 1:
        time_diffs = np.diff(times_numeric)
        features['avg_time_between_moves'] = np.mean(time_diffs)
        features['std_time_between_moves'] = np.std(time_diffs)
        features['min_time_between_moves'] = np.min(time_diffs)
        features['max_time_between_moves'] = np.max(time_diffs)
        features['total_session_duration'] = times_numeric[-1] - times_numeric[0]
    else:
        features['avg_time_between_moves'] = 0
        features['std_time_between_moves'] = 0
        features['min_time_between_moves'] = 0
        features['max_time_between_moves'] = 0
        features['total_session_duration'] = 0

    coords = []
    if mousemove_total_behaviour:
        for coord_str in mousemove_total_behaviour:
            try:
                x, y = map(int, coord_str.strip('()').split(','))
                coords.append((x, y))
            except ValueError:
                continue

    if len(coords) > 1:
        x_coords = np.array([c[0] for c in coords])
        y_coords = np.array([c[1] for c in coords])

        x_diffs = np.diff(x_coords)
        y_diffs = np.diff(y_coords)
        distances = np.sqrt(x_diffs**2 + y_diffs**2)

        features['total_distance'] = np.sum(distances)
        features['avg_speed'] = features['total_distance'] / features['total_session_duration'] if features['total_session_duration'] > 0 else 0
        if len(time_diffs) > 0 and (distances / time_diffs).size > 0:
            features['std_speed'] = np.std(distances / time_diffs)
        else:
            features['std_speed'] = 0

        features['straightness'] = np.sqrt((x_coords[-1] - x_coords[0])**2 + (y_coords[-1] - y_coords[0])**2) / features['total_distance'] if features['total_distance'] > 0 else 0

        features['min_x'] = np.min(x_coords)
        features['max_x'] = np.max(x_coords)
        features['min_y'] = np.min(y_coords)
        features['max_y'] = np.max(y_coords)
        features['std_x'] = np.std(x_coords)
        features['std_y'] = np.std(y_coords)
    else:
        features['total_distance'] = 0
        features['avg_speed'] = 0
        features['std_speed'] = 0
        features['straightness'] = 0
        features['min_x'], features['max_x'], features['min_y'], features['max_y'], features['std_x'], features['std_y'] = 0,0,0,0,0,0

    return features

def extract_web_log_features(web_logs_list):
    """Extracts features from a list of parsed web log entries for a session."""
    features = {}
    if not web_logs_list:
        return {
            'num_requests': 0, 'num_get': 0, 'num_post': 0,
            'unique_paths': 0, 'avg_status_code': 0, 'avg_bytes_sent': 0,
            'num_200_ok': 0, 'num_404_not_found': 0, 'num_redirects': 0,
            'session_duration_web_logs': 0, 'user_agent_diversity': 0
        }

    num_requests = len(web_logs_list)
    features['num_requests'] = num_requests
    features['num_get'] = sum(1 for log in web_logs_list if log['method'] == 'GET')
    features['num_post'] = sum(1 for log in web_logs_list if log['method'] == 'POST')
    features['unique_paths'] = len(set(log['path'] for log in web_logs_list))
    features['avg_status_code'] = np.mean([log['status_code'] for log in web_logs_list])
    features['avg_bytes_sent'] = np.mean([log['bytes_sent'] for log in web_logs_list])
    features['num_200_ok'] = sum(1 for log in web_logs_list if log['status_code'] == 200)
    features['num_404_not_found'] = sum(1 for log in web_logs_list if log['status_code'] == 404)
    features['num_redirects'] = sum(1 for log in web_logs_list if log['status_code'] >= 300 and log['status_code'] < 400)

    timestamps = []
    for log in web_logs_list:
        try:
            timestamps.append(pd.to_datetime(log['timestamp_str'], format='%d/%b/%Y:%H:%M:%S %z'))
        except ValueError:
            pass

    if len(timestamps) > 1:
        features['session_duration_web_logs'] = (timestamps[-1] - timestamps[0]).total_seconds()
    else:
        features['session_duration_web_logs'] = 0

    features['user_agent_diversity'] = len(set(log['user_agent'] for log in web_logs_list))

    return features

# --- Client Logic ---

def load_client_data(client_id):
    """Loads and preprocesses data for a single client, including all bot types as per dataset structure."""
    client_base_path = os.path.join(BASE_PARTITION_DIR, client_id, PHASE)

    all_annotations_dfs = []
    session_ids_to_process = set()

    annotation_subfolders = ['humans_and_advanced_bots', 'humans_and_moderate_bots']

    for current_subfolder in annotation_subfolders:
        train_annotation_path = os.path.join(client_base_path, 'annotations', current_subfolder, 'train')
        test_annotation_path = os.path.join(client_base_path, 'annotations', current_subfolder, 'test')

        if os.path.exists(train_annotation_path):
            all_annotations_dfs.append(pd.read_csv(train_annotation_path, sep=' ', header=None, names=['session_id', 'label']))
        if os.path.exists(test_annotation_path):
            all_annotations_dfs.append(pd.read_csv(test_annotation_path, sep=' ', header=None, names=['session_id', 'label']))

    if not all_annotations_dfs:
        print(f"[{client_id}] No annotation files found. Cannot load data.")
        return pd.DataFrame(), pd.Series(), []

    annotations_df = pd.concat(all_annotations_dfs).drop_duplicates(subset=['session_id'])
    session_ids_to_process.update(annotations_df['session_id'].tolist())

    print(f"[{client_id}] Loaded {len(session_ids_to_process)} unique sessions from all annotation types.")

    all_web_logs = {}
    web_log_base_path = os.path.join(client_base_path, 'data', 'web_logs')

    log_subfolders = ['bots', 'humans']
    for subfolder in log_subfolders:
        current_log_path = os.path.join(web_log_base_path, subfolder)
        if os.path.exists(current_log_path):
            log_files = [f for f in os.listdir(current_log_path) if f.endswith('.log')]
            for log_file in tqdm(log_files, desc=f"[{client_id}] Reading {subfolder} Web Logs"):
                file_path = os.path.join(current_log_path, log_file)
                with open(file_path, 'r') as f:
                    for line in f:
                        parsed_log = parse_web_log_entry(line.strip())
                        if parsed_log and parsed_log['session_id'] in session_ids_to_process:
                            if parsed_log['session_id'] not in all_web_logs:
                                all_web_logs[parsed_log['session_id']] = []
                            all_web_logs[parsed_log['session_id']].append(parsed_log)
        else:
            print(f"[{client_id}] Web logs directory not found: {current_log_path}")

    all_mouse_movements = {}
    mouse_movement_data_base_path = os.path.join(client_base_path, 'data', 'mouse_movements')

    for current_mm_type in annotation_subfolders:
        mouse_movement_path = os.path.join(mouse_movement_data_base_path, current_mm_type)
        if os.path.exists(mouse_movement_path):
            session_folders = [d for d in os.listdir(mouse_movement_path) if os.path.isdir(os.path.join(mouse_movement_path, d))]
            for session_folder in tqdm(session_folders, desc=f"[{client_id}] Reading {current_mm_type} Mouse Movements"):
                session_id = session_folder
                if session_id in session_ids_to_process:
                    json_file_path = os.path.join(mouse_movement_path, session_folder, 'mouse_movements.json')
                    if os.path.exists(json_file_path):
                        with open(json_file_path, 'r') as f:
                            try:
                                mouse_data = json.load(f)
                                all_mouse_movements[session_id] = mouse_data
                            except json.JSONDecodeError:
                                print(f"[{client_id}] Error decoding JSON for session {session_id} in {current_mm_type}")
                    else:
                        print(f"[{client_id}] Warning: mouse_movements.json not found for session {session_id} in {current_mm_type}")
        else:
            print(f"[{client_id}] Mouse movements directory not found: {mouse_movement_path}")

    features_list = []
    labels_list = []

    print(f"[{client_id}] Extracting features for all combined sessions...")
    for session_id in tqdm(list(session_ids_to_process), desc=f"[{client_id}] Extracting Features"):
        mouse_feats = extract_mouse_movement_features(all_mouse_movements.get(session_id, {}))
        web_log_feats = extract_web_log_features(all_web_logs.get(session_id, []))

        label_row = annotations_df[annotations_df['session_id'] == session_id]
        if label_row.empty:
            continue

        label = label_row['label'].iloc[0]

        combined_features = {'session_id': session_id}
        combined_features.update(mouse_feats)
        combined_features.update(web_log_feats)

        features_list.append(combined_features)
        labels_list.append(label)

    client_data_df = pd.DataFrame(features_list)
    client_data_df['label'] = labels_list

    label_mapping = {'human': 0, 'moderate_bot': 1, 'advanced_bot': 2}
    client_data_df['label_encoded'] = client_data_df['label'].map(label_mapping)

    X = client_data_df.drop(columns=['session_id', 'label', 'label_encoded'])
    y = client_data_df['label_encoded']

    X = X.fillna(0)
    for col in X.columns:
        if X[col].dtype == 'object':
            try:
                X[col] = pd.to_numeric(X[col])
            except ValueError:
                X = X.drop(columns=[col])

    return X, y, list(X.columns)

def apply_differential_privacy(data_array, noise_scale):
    """
    Applies Gaussian noise for differential privacy.
    data_array: The numpy array of model parameters (e.g., feature importances).
    noise_scale: The standard deviation of the Gaussian noise.
    """
    noise = np.random.normal(loc=0, scale=noise_scale, size=data_array.shape)
    noisy_data = data_array + noise
    return noisy_data

def encrypt_data(data, key):
    """Encrypts data using Fernet symmetric encryption."""
    f = Fernet(key)
    serialized_data = pickle.dumps(data)
    encrypted_data = f.encrypt(serialized_data)
    return encrypted_data

def run_client_training(client_id, round_num, global_model_params=None):
    """
    Main function to run a single client's FL round.
    """
    print(f"\n--- Client {client_id} (Round {round_num}) ---")

    X, y, feature_names = load_client_data(client_id)

    if X.empty or len(y.unique()) < 2:
        print(f"[{client_id}] Not enough data or classes to train model. Skipping round.")
        return None

    if global_model_params and 'feature_names' in global_model_params:
        expected_features = global_model_params['feature_names']
        for feat in expected_features:
            if feat not in X.columns:
                X[feat] = 0
        X = X[expected_features]
        feature_names = expected_features

    print(f"[{client_id}] Training local model...")
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    model.fit(X_train, y_train)

    y_pred = model.predict(X_val)
    print(f"[{client_id}] Local Model Performance (Round {round_num}):")
    print(f"Accuracy: {accuracy_score(y_val, y_pred):.4f}")

    original_importances = np.array(model.feature_importances_)
    noisy_importances = apply_differential_privacy(original_importances, DP_NOISE_SCALE)
    print(f"[{client_id}] Applied differential privacy (noise scale: {DP_NOISE_SCALE}).")

    model_update_payload = {
        'client_id': client_id,
        'feature_importances': noisy_importances.tolist(),
        'num_samples': len(X_train),
        'feature_names': feature_names
    }

    encrypted_update = encrypt_data(model_update_payload, ENCRYPTION_KEY)
    print(f"[{client_id}] Encrypted model update.")

    # Modified: Save encrypted update to the new client_updates directory
    os.makedirs(CLIENT_UPDATES_DIR, exist_ok=True) # Ensure directory exists
    update_filename = os.path.join(CLIENT_UPDATES_DIR, f"client_update_{client_id}_round_{round_num}.enc")
    with open(update_filename, 'wb') as f:
        f.write(encrypted_update)
    print(f"[{client_id}] Encrypted model update saved to {update_filename}")

    return model_update_payload

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python client.py <client_id> [round_num]")
        sys.exit(1)

    client_id = sys.argv[1]
    round_num = int(sys.argv[2]) if len(sys.argv) > 2 else 1

    # Modified: Load global model from the new global_models directory
    global_params_path = os.path.join('global_models', f"global_model_params_round_{round_num-1}.pkl") if round_num > 1 else None
    global_params = None
    if global_params_path and os.path.exists(global_params_path):
        with open(global_params_path, 'rb') as f:
            global_params = pickle.load(f)

    run_client_training(client_id, round_num, global_params)